{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br9kJq5hbWdm"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.vector_ar.var_model import VAR\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "\n",
        "# Dataset Generation\n",
        "def generate_multivariate_ts(n_steps=3000, n_features=6, noise_std=0.08):\n",
        "    t = np.arange(n_steps)\n",
        "    dates = pd.date_range(start=\"2000-01-01\", periods=n_steps, freq='D')\n",
        "    data = np.zeros((n_steps, n_features))\n",
        "\n",
        "    base_trend = 0.0005 * t\n",
        "    weekly = np.sin(2 * np.pi * t / 7)\n",
        "    annual = np.sin(2 * np.pi * t / 365.25)\n",
        "\n",
        "    for i in range(n_features):\n",
        "        data[:, i] = (\n",
        "            (i + 1) * 0.2 * base_trend +\n",
        "            0.4 * weekly * (1 / (i + 1)) +\n",
        "            0.3 * annual * ((i % 3) + 1)\n",
        "        )\n",
        "\n",
        "        ar = np.zeros(n_steps)\n",
        "        phi = 0.6 - 0.05 * i\n",
        "        ar[0] = np.random.normal(scale=noise_std)\n",
        "        for j in range(1, n_steps):\n",
        "            ar[j] = phi * ar[j - 1] + np.random.normal(scale=noise_std * (1 + 0.1 * i))\n",
        "        data[:, i] += ar\n",
        "        data[:, i] += np.random.normal(scale=noise_std, size=n_steps)\n",
        "\n",
        "    df = pd.DataFrame(data, index=dates, columns=[f\"f{i+1}\" for i in range(n_features)])\n",
        "    return df\n",
        "\n",
        "\n",
        "df = generate_multivariate_ts()\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Split & Scaling\n",
        "def train_val_test_split(df, train_frac=0.7, val_frac=0.15):\n",
        "    n = len(df)\n",
        "    train = df.iloc[: int(n * train_frac)]\n",
        "    val = df.iloc[int(n * train_frac): int(n * (train_frac + val_frac))]\n",
        "    test = df.iloc[int(n * (train_frac + val_frac)):]\n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "train_df, val_df, test_df = train_val_test_split(df)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(pd.concat([train_df, val_df]))\n",
        "\n",
        "train_s = pd.DataFrame(scaler.transform(train_df), index=train_df.index)\n",
        "val_s = pd.DataFrame(scaler.transform(val_df), index=val_df.index)\n",
        "test_s = pd.DataFrame(scaler.transform(test_df), index=test_df.index)\n",
        "\n",
        "\n",
        "# Sequence Creation\n",
        "def create_sequences(data, input_len=30, horizon=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - input_len - horizon + 1):\n",
        "        X.append(data[i:i+input_len])\n",
        "        y.append(data[i+input_len : i+input_len+horizon])\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    if horizon == 1:\n",
        "        y = y.reshape(len(y), -1)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# LSTM Model\n",
        "def build_lstm(input_len, n_features, units=64, n_layers=1, dropout_rate=0.2, lr=1e-3):\n",
        "    inp = keras.Input(shape=(input_len, n_features))\n",
        "    x = inp\n",
        "    for i in range(n_layers):\n",
        "        return_seq = (i < n_layers - 1)\n",
        "        x = layers.LSTM(units, return_sequences=return_seq)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    out = layers.Dense(n_features)(x)\n",
        "    model = keras.Model(inp, out)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr), loss=\"mse\", metrics=[\"mae\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Manual Grid Search\n",
        "def manual_grid_search(train_scaled):\n",
        "    results = []\n",
        "    input_lens = [20, 30]\n",
        "    units_list = [32, 64]\n",
        "    dropouts = [0.1, 0.3]\n",
        "\n",
        "    for il in input_lens:\n",
        "        X, y = create_sequences(train_scaled.values, il)\n",
        "        split = int(0.8 * len(X))\n",
        "        X_tr, X_val = X[:split], X[split:]\n",
        "        y_tr, y_val = y[:split], y[split:]\n",
        "\n",
        "        for u in units_list:\n",
        "            for d in dropouts:\n",
        "                model = build_lstm(il, train_scaled.shape[1], units=u, dropout_rate=d)\n",
        "                h = model.fit(X_tr, y_tr, validation_data=(X_val, y_val),\n",
        "                              epochs=5, batch_size=64, verbose=0)\n",
        "                val_loss = h.history[\"val_loss\"][-1]\n",
        "                results.append((il, u, d, val_loss))\n",
        "    return results\n",
        "\n",
        "\n",
        "# MC Dropout Sampling\n",
        "def mc_dropout_predict(model, x, samples=100):\n",
        "    preds = []\n",
        "    for _ in range(samples):\n",
        "        preds.append(model(x, training=True).numpy())\n",
        "    return np.stack(preds)\n",
        "\n",
        "\n",
        "# MC Intervals\n",
        "def mc_intervals(mc_preds, alpha=0.1):\n",
        "    lower = np.quantile(mc_preds, alpha/2, axis=0)\n",
        "    upper = np.quantile(mc_preds, 1 - alpha/2, axis=0)\n",
        "    median = np.median(mc_preds, axis=0)\n",
        "    return lower, upper, median\n",
        "\n",
        "\n",
        "# Rolling Forecast\n",
        "def rolling_forecast(model, history_scaled, input_len, steps_out, samples=100):\n",
        "    preds, lowers, uppers = [], [], []\n",
        "    history = history_scaled.copy()\n",
        "\n",
        "    for _ in range(steps_out):\n",
        "        window = history[-input_len:]\n",
        "        x = window.reshape(1, input_len, -1)\n",
        "\n",
        "        mc = mc_dropout_predict(model, x, samples=samples)\n",
        "        lower, upper, median = mc_intervals(mc)\n",
        "\n",
        "        preds.append(median.ravel())\n",
        "        lowers.append(lower.ravel())\n",
        "        uppers.append(upper.ravel())\n",
        "\n",
        "        history = np.vstack([history, median])\n",
        "\n",
        "    return np.array(preds), np.array(lowers), np.array(uppers)\n",
        "\n",
        "\n",
        "# VAR Baseline\n",
        "def var_baseline(train_df, steps):\n",
        "    model = VAR(train_df)\n",
        "    res = model.fit(ic=\"aic\")\n",
        "    p = res.k_ar\n",
        "    last = train_df.values[-p:]\n",
        "    forecast = res.forecast(last, steps=steps)\n",
        "    return forecast\n",
        "\n",
        "\n",
        "# Metrics\n",
        "def rmse(a, p): return np.sqrt(mean_squared_error(a, p))\n",
        "def mae(a, p): return mean_absolute_error(a, p)\n",
        "def interval_coverage(actual, lower, upper):\n",
        "    return ((actual >= lower) & (actual <= upper)).mean()\n",
        "\n",
        "\n",
        "# Pipeline\n",
        "def run_pipeline(df):\n",
        "    input_len = 30\n",
        "    n_features = df.shape[1]\n",
        "\n",
        "    full_train = pd.concat([train_s, val_s])\n",
        "    X_train, y_train = create_sequences(full_train.values, input_len)\n",
        "\n",
        "    model = build_lstm(input_len, n_features, units=64, dropout_rate=0.2)\n",
        "    model.fit(X_train, y_train, epochs=25, batch_size=64, verbose=1)\n",
        "\n",
        "    pred_med, lower, upper = rolling_forecast(\n",
        "        model, full_train.values, input_len, len(test_s), samples=200\n",
        "    )\n",
        "\n",
        "    pred_med_un = scaler.inverse_transform(pred_med)\n",
        "    lower_un = scaler.inverse_transform(lower)\n",
        "    upper_un = scaler.inverse_transform(upper)\n",
        "    actual_un = scaler.inverse_transform(test_s)\n",
        "\n",
        "    metrics = {}\n",
        "    metrics[\"LSTM_RMSE\"] = rmse(actual_un, pred_med_un)\n",
        "    metrics[\"LSTM_MAE\"] = mae(actual_un, pred_med_un)\n",
        "    metrics[\"LSTM_Interval_Coverage\"] = interval_coverage(actual_un, lower_un, upper_un)\n",
        "\n",
        "    var_fc = var_baseline(train_df, len(test_df))\n",
        "    metrics[\"VAR_RMSE\"] = rmse(test_df.values, var_fc)\n",
        "    metrics[\"VAR_MAE\"] = mae(test_df.values, var_fc)\n",
        "\n",
        "    return model, pred_med_un, lower_un, upper_un, actual_un, var_fc, metrics\n",
        "\n",
        "\n",
        "# Run\n",
        "model, pred, lower, upper, actual, var_fc, metrics = run_pipeline(df)\n",
        "\n",
        "print(\"\\n=== FINAL METRICS ===\")\n",
        "for k, v in metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.plot(actual[:,0], label=\"Actual (f1)\", color=\"black\")\n",
        "plt.plot(pred[:,0], label=\"LSTM Prediction (f1)\", color=\"blue\")\n",
        "plt.fill_between(np.arange(len(pred)), lower[:,0], upper[:,0], color=\"cyan\", alpha=0.3)\n",
        "plt.title(\"LSTM Forecast + MC Dropout Interval (Feature 1)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}